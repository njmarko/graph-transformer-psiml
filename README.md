# graph-transformer-psiml
Transformer implemented with graph neural network attention layer from Pytorch Geometric. This was a project for [PSIML](https://psiml.petlja.org/), Practical Seminar for Machine Learning organized by PFE, Petlja, Everseen and Microsoft in Belgrade 2022.

<div align="center">
  <img src="https://user-images.githubusercontent.com/34657562/184308361-554b6ce6-5cac-4f99-94c0-66bb48864d69.png" align="center" width="50%">
</div>

## Authors

- Marina Debogović (ETF)
- Marko Njegomir (FTN)

## Mentors
- Anđela Donević (Everseen)
- Nikola Popović (ETH Zurich)

<div align="center">
  <img src="https://user-images.githubusercontent.com/34657562/184306183-802cb780-29ce-4fed-95b6-82023b199354.png">
  <p align="center">Illustration - Transformer with graph attention network (DALLE-2).</p>
</div>

## Results MNIST

<div align="center">
  <img src="images/mnist-train-loss.png">
  <p align="center">Illustration - MNIST train loss for Classic ViT and out Grpah Transformer.</p>
</div>

<div align="center">
  <img src="images/mnist-train-acc.png">
  <p align="center">Illustration - MNIST train accuracy for Classic ViT and out Grpah Transformer.</p>
</div>

<div align="center">
  <img src="images/mnist-val-acc.png">
  <p align="center">Illustration - MNIST validation accuracy for Classic ViT and out Grpah Transformer.</p>
</div>
